{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid 19 data analysis\n",
    "This notebook contains Python code that perform exploratory data analysis (EDA) using Apache Spark\n",
    "Data Analytics engine and Pandas\n",
    "\n",
    "**Requirements**\n",
    "- Python 3.7 or below.\n",
    "- Apache Spark\n",
    "- Pandas\n",
    "\n",
    "**Other tools**\n",
    "- scipy\n",
    "- numpy\n",
    "- matplotlib\n",
    "- scikit-learn\n",
    "- seaborn\n",
    "- scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from hdfs3 import HDFileSystem\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '12-28-2020'\n",
    "covid_data_dir='hdfs://localhost:9000/user/student/csse_covid_19_data/'\n",
    "covid_data_daily_reports_dir=covid_data_dir + 'csse_covid_19_daily_reports/'\n",
    "covid_data_daily_file=covid_data_daily_reports_dir + date + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"covid-stats\").config(\"spark.config.option\", \"value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Apache Spark version {spark.version}')\n",
    "print(f'Daily exploratory Data Analysis (EDA) for file {covid_data_daily_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading daily raw data from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.option(\"header\", \"true\").csv(covid_data_daily_file)\n",
    "print(f'Load csv file successfully. There are {df_raw.count()} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_by_country = df_raw.groupby(['Country Region'])\n",
    "df_clean = df_raw.select([c for c in df_raw.columns if c in ['Country_Region','Confirmed','Deaths', 'Active', 'Recovered']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data by countries or regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toInt(item):\n",
    "    if item:\n",
    "        return int(item)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed = df_clean.rdd.map(lambda x: (x['Country_Region'], toInt(x['Confirmed']))).reduceByKey(lambda x,y: x + y).toDF()\n",
    "df_deaths    = df_clean.rdd.map(lambda x: (x['Country_Region'], toInt(x['Deaths']))).reduceByKey(lambda x,y: x + y).toDF()\n",
    "df_recovered = df_clean.rdd.map(lambda x: (x['Country_Region'], toInt(x['Recovered']))).reduceByKey(lambda x,y: x + y).toDF()\n",
    "df_active    = df_clean.rdd.map(lambda x: (x['Country_Region'], toInt(x['Active']))).reduceByKey(lambda x,y: x + y).toDF()\n",
    "\n",
    "df_confirmed = df_confirmed.selectExpr(\"_1 as Country_Region\", \"_2 as Confirmed\")\n",
    "df_deaths    = df_deaths.selectExpr(\"_1 as Country_Region\", \"_2 as Deaths\")\n",
    "df_recovered = df_recovered.selectExpr(\"_1 as Country_Region\", \"_2 as Recovered\")\n",
    "df_active    = df_active.selectExpr(\"_1 as Country_Region\", \"_2 as Active\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new = df_confirmed.join(df_deaths, on=['Country_Region'], how='left_outer')\n",
    "df_new = df_new.join(df_recovered, on=['Country_Region'], how='left_outer')\n",
    "df_new = df_new.join(df_active, on=['Country_Region'], how='left_outer')\n",
    "df_by_country = df_new\n",
    "df_by_country.describe().show()\n",
    "# df_by_country.where(df_new.Country_Region == 'Zimbabwe').collect()\n",
    "df_by_country.where(df_by_country.Country_Region == 'US').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Confirmed:\n",
    "# df_by_country.select(\"Confirmed\").rdd.max()[0]\n",
    "for col in df_by_country.columns:\n",
    "    if col != 'Country_Region':\n",
    "        rdd = df_by_country.select(col).rdd\n",
    "        print(rdd.max(), rdd.min())\n",
    "# df_by_country.where(df_by_country.Confirmed == df_by_country['Confirmed'].max()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Writing to file ordered by country\n",
    "\n",
    "df_output_filename = '/tmp/' + date\n",
    "subprocess.run(['hdfs', 'dfs', '-rm', '-r', df_output_filename])\n",
    "df_by_country.orderBy('Country_Region')\\\n",
    "             .coalesce(1)\\\n",
    "             .write.format('csv')\\\n",
    "             .option('header','true')\\\n",
    "             .save(df_output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_country.orderBy('Country_Region').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Spark Dataframe to PANDAS Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country = df_by_country.toPandas()\n",
    "pd_by_country[(pd_by_country.Confirmed == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.idxmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, content in pd_by_country.items():\n",
    "    print(f'label: {label}')\n",
    "    print(f'content: {content}', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country[(pd_by_country.Country_Region == 'US')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_by_country.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country.to_json('./country_by_day.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean:', pd_by_country['Confirmed'].mean())\n",
    "print('median:', pd_by_country['Confirmed'].median())\n",
    "print('min:', pd_by_country['Confirmed'].min())\n",
    "print('stddev:', pd_by_country['Confirmed'].std())\n",
    "print('kurtosis:', pd_by_country['Confirmed'].kurtosis())\n",
    "print('skew:', pd_by_country['Confirmed'].skew())\n",
    "print('idxmax:', pd_by_country['Confirmed'].idxmax())\n",
    "print('idxmin:', pd_by_country['Confirmed'].idxmin())\n",
    "print('var:', pd_by_country['Confirmed'].var())\n",
    "# pd_by_country['Confirmed'].plot()\n",
    "# pd_by_country['Deaths'].plot()\n",
    "# pd_by_country['Recovered'].plot()\n",
    "pd_by_country.plot(figsize=(15,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
